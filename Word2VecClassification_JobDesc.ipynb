{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe801480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, gensim, logging, string, re, operator, pdb\n",
    "from scipy import spatial\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import csv\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "word2vec_model = None\n",
    "job_description = None\n",
    "word2vec_file = './data/googlenews.bin.gz'\n",
    "occupation_file = './data/OccupationData.tsv'\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c73aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(str):\n",
    "    return regex.sub(' ', str)\n",
    "\n",
    "def load_word2vec(fname):\n",
    "    ''' load a pre-trained binary format word2vec into a dictionary\n",
    "    the model is downloaded from https://docs.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download'''\n",
    "    word2vec = gensim.models.KeyedVectors.load_word2vec_format(fname, binary=True)\n",
    "    return word2vec\n",
    "\n",
    "def load_jobs(fname):\n",
    "    ''' read ONET occupational dataset from tab delimited text file downloaded from\n",
    "    https://www.onetcenter.org/dl_files/database/db_21_0_text/Occupation%20Data.txt'''\n",
    "    \n",
    "    jobtitle_jobdescription = {}\n",
    "    with codecs.open(fname, 'r', encoding='utf-8') as fin:\n",
    "        for line in fin:\n",
    "            fields = line.strip().split('\\t')\n",
    "            if len(fields) != 3:\n",
    "                continue\n",
    "            job_code = fields[0]\n",
    "            job_title = remove_punctuation(fields[1].lower())\n",
    "            _job_description = remove_punctuation(fields[2].lower())\n",
    "            jobtitle_jobdescription[job_title] = _job_description\n",
    "    return jobtitle_jobdescription\n",
    "\n",
    "def idtext2vec(id_text, word2vec_model):\n",
    "    '''convert a dictionary of id:text to text_id:vector by averaging the word vectors'''\n",
    "    id_vec = {}\n",
    "    for id, text in id_text.items():\n",
    "        vec = text2vec(text, word2vec_model)\n",
    "        id_vec[id] = vec\n",
    "    return id_vec\n",
    "\n",
    "def text2vec(text, word2vec_model):\n",
    "    '''convert a text to a vector by averaging the word vectors'''\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    vec = 0\n",
    "    num_words = 0\n",
    "    for word in words:\n",
    "        if word in word2vec_model:\n",
    "            num_words += 1\n",
    "            vec += word2vec_model[word]\n",
    "    if num_words == 0:\n",
    "        vec = np.asarray([0] * 300)\n",
    "    else:\n",
    "        vec = vec / num_words\n",
    "    return vec\n",
    "\n",
    "def textsimilarity(text_pairs, word2vec_model):\n",
    "    text_similarity_features = []\n",
    "    for text_pair in text_pairs:\n",
    "        text1, text2 = text_pair\n",
    "        vec1 = text2vec(text1, word2vec_model)\n",
    "        vec2 = text2vec(text2, word2vec_model)\n",
    "        similarity = 1 - spatial.distance.cosine(vec1, vec2)\n",
    "        text_similarity_features.append(similarity)\n",
    "    features = np.asarray(text_similarity_features).reshape(len(text_similarity_features), 1)\n",
    "    return features\n",
    "        \n",
    "\n",
    "def sort_dic_by_value(dic):\n",
    "    sorted_x = sorted(dic.items(), key=operator.itemgetter(1))\n",
    "    return OrderedDict(sorted_x)\n",
    "\n",
    "def get_job_dict_ordered(id_text1, id_text2, word2vec_model):\n",
    "    id_vec1 = idtext2vec(id_text1, word2vec_model)\n",
    "    id_vec2 = idtext2vec(id_text2, word2vec_model)\n",
    "    id1_id2distances = {}\n",
    "    for id1, vec1 in id_vec1.items():\n",
    "        id2_distances = {}\n",
    "        for id2, vec2 in id_vec2.items():\n",
    "            distance = spatial.distance.cosine(vec1, vec2)\n",
    "            id2_distances[id2] = distance\n",
    "        id1_id2distances[id1] = sort_dic_by_value(id2_distances)\n",
    "    return id1_id2distances\n",
    "\n",
    "\n",
    "def get_features(text_pairs, jobtitle_jobdesc, word2vec_model):\n",
    "    '''given a list of text pairs as [('t11', 't12'), ('t21', 't22')....]\n",
    "    returns features, a vector where the first element is the job similarity of 't11', 't12'.\n",
    "    The length of the features vector equals the length of the pairs.'''\n",
    "    jobtitle_vec = idtext2vec(jobtitle_jobdesc, word2vec_model)\n",
    "    jobtitles = sorted(set(jobtitle_vec.keys()))\n",
    "    features = []\n",
    "    for text_pair in text_pairs:\n",
    "        text1, text2 = text_pair\n",
    "        vec1 = text2vec(text1, word2vec_model)\n",
    "        vec2 = text2vec(text2, word2vec_model)\n",
    "        vec1distances = []\n",
    "        vec2distances = []\n",
    "        for jobtitle in jobtitles:\n",
    "            vec = jobtitle_vec[jobtitle]\n",
    "            distance1 = spatial.distance.cosine(vec1, vec)\n",
    "            distance2 = spatial.distance.cosine(vec2, vec)\n",
    "            vec1distances.append(distance1)\n",
    "            vec2distances.append(distance2)\n",
    "        jobsim = 1 - spatial.distance.cosine(vec1distances, vec2distances)\n",
    "        features.append(jobsim)\n",
    "    features = np.asarray(features).reshape(len(features), 1)\n",
    "    return features\n",
    "\n",
    "def normalize_features(train_features, test_features):\n",
    "    ''' scale the feature values '''\n",
    "    #scaler = preprocessing.StandardScaler()\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    scaler.fit(train_features)\n",
    "    normal_train = scaler.transform(train_features)\n",
    "    test_features = scaler.transform(test_features)\n",
    "    return train_features, test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1009bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def import_train_test_data(train_file, test_file):\n",
    "#     html_dir = './data/html_clean/'\n",
    "    train_pairs = {}\n",
    "    test_pairs = {}\n",
    "\n",
    "    # Load train file\n",
    "    with open(train_file, 'r') as fin:\n",
    "        reader = csv.reader(fin, delimiter=',', quotechar='\"')\n",
    "        header = True\n",
    "        for row in reader:\n",
    "            if header:\n",
    "                header = False\n",
    "                continue\n",
    "\n",
    "            pair_id = int(row[0])\n",
    "            title1 = row[2]\n",
    "            title2 = row[5]\n",
    "            text1 = row[3]\n",
    "            text2 = row[6]\n",
    "            train_pairs[pair_id] = (title1 + ' ' + text1 , title2 + ' ' + text2)\n",
    "\n",
    "    # Load test file\n",
    "    with open(test_file, 'r') as fin:\n",
    "        reader = csv.reader(fin, delimiter=',', quotechar='\"')\n",
    "        header = True\n",
    "        for row in reader:\n",
    "            if header:\n",
    "                header = False\n",
    "                continue\n",
    "\n",
    "            pair_id = int(row[0])\n",
    "            title1 = row[2]\n",
    "            title2 = row[5]\n",
    "            text1 = row[3]\n",
    "            text2 = row[6]\n",
    "            test_pairs[pair_id] = (title1 + ' ' + text1 , title2 + ' ' + text2)\n",
    "    return train_pairs, test_pairs        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f929ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(word2vec_model, job_description):\n",
    "    #just for sanity check\n",
    "    text_job_distances = get_job_dict_ordered({1:'i am a computer programmer'}, job_description, word2vec_model)\n",
    "    print(text_job_distances[1].keys()[0:30])    \n",
    "\n",
    "def write_features(feature_file, features):\n",
    "    np.savetxt(feature_file, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf30906b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 12:00:37,580 : INFO : loading train and test search results...\n",
      "2023-03-15 12:00:37,585 : INFO : loading job descriptions...\n",
      "2023-03-15 12:00:37,594 : INFO : loading word2vec model (takes a while)...\n",
      "2023-03-15 12:00:37,595 : INFO : loading projection weights from ./data/googlenews.bin.gz\n",
      "2023-03-15 12:01:10,927 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from ./data/googlenews.bin.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-03-15T12:01:10.927714', 'gensim': '4.3.1', 'python': '3.10.9 (main, Dec 19 2022, 17:35:49) [GCC 12.2.0]', 'platform': 'Linux-6.2.2-arch1-1-x86_64-with-glibc2.37', 'event': 'load_word2vec_format'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m/tmp/ipykernel_264480/538680874.py\u001b[0m(21)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     17 \u001b[0;31m    \u001b[0;31m#test_features = preprocessing.scale(test_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     18 \u001b[0;31m    \u001b[0;31m#train_features, test_features = normalize_features(train_features=train_features, test_features=test_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     19 \u001b[0;31m    \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     20 \u001b[0;31m    \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/features.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 21 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> get\n",
      "*** NameError: name 'get' is not defined\n",
      "ipdb> np\n",
      "<module 'numpy' from '/home/joel/.local/lib/python3.10/site-packages/numpy/__init__.py'>\n",
      "ipdb> np.read\n",
      "*** AttributeError: module 'numpy' has no attribute 'read'\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "if __name__ == '__main__':\n",
    "    logging.info('loading train and test search results...')\n",
    "    train_pairs, test_pairs = import_train_test_data(train_file='./data/alta16_kbcoref_train_search_results.csv', test_file='./data/alta16_kbcoref_test_search_results.csv')\n",
    "    logging.info('loading job descriptions...')\n",
    "    job_description = load_jobs(fname=occupation_file)\n",
    "    #add job title to job description\n",
    "    job_description = {job:job + ' ' + desc for job, desc in job_description.items()}\n",
    "    logging.info('loading word2vec model (takes a while)...')\n",
    "    word2vec_model = load_word2vec(fname=word2vec_file)\n",
    "    train_features_job = get_features(text_pairs=[train_pairs[id] for id in sorted(train_pairs.keys())], jobtitle_jobdesc=job_description, word2vec_model=word2vec_model) \n",
    "    test_features_job = get_features(text_pairs=[test_pairs[id] for id in sorted(test_pairs.keys())], jobtitle_jobdesc=job_description, word2vec_model=word2vec_model)\n",
    "    train_features_txtsim = textsimilarity(text_pairs=[train_pairs[id] for id in sorted(train_pairs.keys())], word2vec_model=word2vec_model)\n",
    "    test_features_txtsim = textsimilarity(text_pairs=[test_pairs[id] for id in sorted(test_pairs.keys())], word2vec_model=word2vec_model)\n",
    "    train_features = np.hstack((train_features_job, train_features_txtsim))\n",
    "    test_features = np.hstack((test_features_job, test_features_txtsim))\n",
    "    train_features = preprocessing.scale(train_features)\n",
    "    #test_features = preprocessing.scale(test_features)\n",
    "    #train_features, test_features = normalize_features(train_features=train_features, test_features=test_features)\n",
    "    features = np.vstack((train_features, test_features))\n",
    "    np.savetxt('./data/features.txt', features)\n",
    "    pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7a09a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
